{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# モジュールをインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 計算用の関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ネイピア数\n",
    "$$e = \\lim_{{x \\to \\infty}} \\left(1 + \\frac{1}{x}\\right)^x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.7182817983473577\n"
     ]
    }
   ],
   "source": [
    "def napiers_logarithm(x):\n",
    "    \"\"\"\n",
    "    ネイピア数を求める関数\n",
    "\n",
    "    Args:\n",
    "        x (float): ネイピア数の底\n",
    "    \n",
    "    Returns:\n",
    "        float: ネイピア数\n",
    "    \"\"\"\n",
    "    return (1 + 1 / x) ** x\n",
    "napier_number = napiers_logarithm(100000000)  # e\n",
    "print(napier_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自然対数の近似値"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3025850929940455\n"
     ]
    }
   ],
   "source": [
    "def ln(x, max_iter=20, tol=1e-12):\n",
    "    \"\"\"\n",
    "    自然対数を求める関数\n",
    "\n",
    "    Args:\n",
    "        x (float): 自然対数を求める値\n",
    "        max_iter (int): 最大反復回数\n",
    "        tol (float): 許容誤差\n",
    "    \n",
    "    Returns:\n",
    "        float: 自然対数\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: x が正でない場合\n",
    "    \"\"\"\n",
    "    if x <= 0: raise ValueError(\"x must be positive\")\n",
    "    k = 0\n",
    "    while x > 2:\n",
    "        x /= 2\n",
    "        k += 1\n",
    "    while x < 0.5:\n",
    "        x *= 2\n",
    "        k -= 1\n",
    "    y = x - 1  # ln(1) = 0 付近の値から開始\n",
    "    for _ in range(max_iter):\n",
    "        prev_y = y\n",
    "        y -= (2.718281828459045**y - x) / (2.718281828459045**y)  # f(y) / f'(y)\n",
    "        if abs(y - prev_y) < tol:\n",
    "            break\n",
    "    return y + k * 0.6931471805599453  # ln(2) ≈ 0.693147\n",
    "print(ln(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 平方根を計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.162277660168379\n"
     ]
    }
   ],
   "source": [
    "def sqrt(x):\n",
    "    \"\"\"\n",
    "    平方根を求める関数\n",
    "\n",
    "    Args:\n",
    "        x (float): 平方根を求める値\n",
    "    \n",
    "    Returns:\n",
    "        float: 平方根\n",
    "    \"\"\"\n",
    "    tolerance = 1e-10  # 許容誤差\n",
    "    estimate = x / 2.0  # 初期推定値\n",
    "    while True:\n",
    "        new_estimate = (estimate + x / estimate) / 2  # ニュートン法による更新\n",
    "        if abs(new_estimate - estimate) < tolerance:  # 収束したら終了\n",
    "            return new_estimate\n",
    "        estimate = new_estimate  # 推定値を更新\n",
    "print(sqrt(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 活性化関数およびその微分関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### シグモイド関数\n",
    "$$Sigmoid(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "### シグモイド関数の微分\n",
    "$$Sigmoid'(x) = Sigmoid(x) \\cdot (1 - Sigmoid(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.999954602126269\n",
      "4.5395812764095245e-05\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x, derivative=False):\n",
    "    \"\"\"\n",
    "    Sigmoid 関数およびその微分\n",
    "\n",
    "    Args:\n",
    "        x (float): 入力\n",
    "        derivative (bool): True の場合は Sigmoid 関数の微分を返す\n",
    "    \n",
    "    Returns:\n",
    "        float: Sigmoid 関数の値またはその微分\n",
    "    \"\"\"\n",
    "    if derivative:\n",
    "        return sigmoid_derivative(x)\n",
    "    return 1 / (1 + napier_number ** -x)\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"\n",
    "    Sigmoid 関数の微分\n",
    "\n",
    "    Args:\n",
    "        x (float): 入力\n",
    "    \n",
    "    Returns:\n",
    "        float: Sigmoid 関数の微分\n",
    "    \"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "print(sigmoid(10, False))\n",
    "print(sigmoid(10, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU関数\n",
    "$$ReLU(x) = \\max(0, x)$$\n",
    "### ReLU関数の微分\n",
    "$$ReLU'(x) = \\begin{cases} \n",
    "1 & (x > 0) \\\\\n",
    "0 & (x ≤ 0)\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def relu(x, derivative=False):\n",
    "    \"\"\"\n",
    "    ReLU 関数およびその微分\n",
    "\n",
    "    Args:\n",
    "        x (float): 入力\n",
    "        derivative (bool): True の場合は ReLU 関数の微分を返す\n",
    "    \n",
    "    Returns:\n",
    "        float: ReLU 関数の値またはその微分\n",
    "    \"\"\"\n",
    "    if derivative:\n",
    "        return relu_derivative(x)\n",
    "    return max(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    \"\"\"\n",
    "    ReLU 関数の微分\n",
    "\n",
    "    Args:\n",
    "        x (float): 入力\n",
    "    \n",
    "    Returns:\n",
    "        float: ReLU 関数の微分\n",
    "    \"\"\"\n",
    "    return 1 if x > 0 else 0\n",
    "print(relu(10, False))\n",
    "print(relu(10, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU関数\n",
    "$$LeakyReLU(x) = \\begin{cases} \n",
    "x & (x > 0) \\\\ \n",
    "\\alpha x & (x ≤ 0)\n",
    "\\end{cases}$$\n",
    "\n",
    "### Leaky ReLU関数の微分\n",
    "$$LeakyReLU'(x) = \\begin{cases} \n",
    "1 & (x > 0) \\\\ \n",
    "\\alpha & (x ≤ 0)\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def leaky_relu(x, derivative=False, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Leaky ReLU 関数およびその微分\n",
    "\n",
    "    Args:\n",
    "        x (float): 入力\n",
    "        alpha (float): ハイパーパラメータ\n",
    "        derivative (bool): True の場合は Leaky ReLU 関数の微分を返す\n",
    "    \n",
    "    Returns:\n",
    "        float: Leaky ReLU 関数の値またはその微分\n",
    "    \"\"\"\n",
    "    if derivative:\n",
    "        return leaky_relu_derivative(x, alpha)\n",
    "    return x if x > 0 else alpha * x\n",
    "\n",
    "def leaky_relu_derivative(x, alpha=0.01):\n",
    "    \"\"\"\n",
    "    Leaky ReLU 関数の微分\n",
    "\n",
    "    Args:\n",
    "        x (float): 入力\n",
    "        alpha (float): ハイパーパラメータ\n",
    "    \n",
    "    Returns:\n",
    "        float: Leaky ReLU 関数の微分\n",
    "    \"\"\"\n",
    "    return 1 if x > 0 else alpha\n",
    "print(leaky_relu(10, False))\n",
    "print(leaky_relu(10, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 恒等関数\n",
    "$$Identity(x) = x$$\n",
    "### 恒等関数の微分\n",
    "$$Identity'(x) = 1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "def identity(x, derivative=False):\n",
    "    \"\"\"\n",
    "    恒等関数およびその微分\n",
    "\n",
    "    Args:\n",
    "        x (float): 入力\n",
    "        derivative (bool): True の場合は恒等関数の微分を返す\n",
    "    \n",
    "    Returns:\n",
    "        float: 恒等関数の値またはその微分\n",
    "    \"\"\"\n",
    "    if derivative:\n",
    "        return identity_derivative(x)\n",
    "    return x\n",
    "\n",
    "def identity_derivative(x):\n",
    "    \"\"\"\n",
    "    恒等関数の微分\n",
    "\n",
    "    Args:\n",
    "        x (float): 入力(未使用)\n",
    "    \n",
    "    Returns:\n",
    "        int: 恒等関数の微分\n",
    "    \"\"\"\n",
    "    return 1\n",
    "print(identity(10, False))\n",
    "print(identity(10, True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 損失関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クロスエントロピー損失\n",
    "$$ L = -\\sum_{i=1}^{N} y_i \\cdot \\ln(\\hat{y}_i + \\epsilon)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10536051454671519\n"
     ]
    }
   ],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    交差エントロピー損失関数\n",
    "\n",
    "    Args:\n",
    "        y_true (list): 正解値\n",
    "        y_pred (list): 予測値\n",
    "    \n",
    "    Returns:\n",
    "        float: 交差エントロピー損失\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: 入力リストの長さが異なる場合\n",
    "    \"\"\"\n",
    "    if len(y_true) != len(y_pred): raise ValueError(\"Input lists must have the same length.\")\n",
    "    return -sum([t * ln(p + 1e-9) for t, p in zip(y_true, y_pred)])\n",
    "print(cross_entropy_loss([0, 1], [0.1, 0.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 平均二乗誤差\n",
    "$$L = \\frac{1}{N} \\sum_{i=1}^{N} (y_i - \\hat{y}_i)^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009999999999999998\n"
     ]
    }
   ],
   "source": [
    "def mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    平均二乗誤差関数\n",
    "\n",
    "    Args:\n",
    "        y_true (list): 正解値\n",
    "        y_pred (list): 予測値\n",
    "    \n",
    "    Returns:\n",
    "        float: 平均二乗誤差\n",
    "        \n",
    "    Raises:\n",
    "        ValueError: 入力リストの長さが異なる場合\n",
    "    \"\"\"\n",
    "    if len(y_true) != len(y_pred): raise ValueError(\"Input lists must have the same length.\")\n",
    "    return sum([(t - p) ** 2 for t, p in zip(y_true, y_pred)]) / len(y_true)\n",
    "print(mean_squared_error([0, 1], [0.1, 0.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 平均絶対誤差\n",
    "$$L = \\frac{1}{N} \\sum_{i=1}^{N} |y_i - \\hat{y}_i|$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09999999999999999\n"
     ]
    }
   ],
   "source": [
    "def mean_absolute_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    平均絶対誤差関数\n",
    "\n",
    "    Args:\n",
    "        y_true (list): 正解値\n",
    "        y_pred (list): 予測値\n",
    "    \n",
    "    Returns:\n",
    "        float: 平均絶対誤差\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: 入力リストの長さが異なる場合\n",
    "    \"\"\"\n",
    "    if len(y_true) != len(y_pred): raise ValueError(\"Input lists must have the same length.\")\n",
    "    return sum([abs(t - p) for t, p in zip(y_true, y_pred)]) / len(y_true)\n",
    "print(mean_absolute_error([0, 1], [0.1, 0.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### バイナリ交差エントロピー\n",
    "$$ L = -\\frac{1}{N} \\sum_{i=1}^{N} \\left[ y_i \\cdot \\ln(\\hat{y}_i + \\epsilon) + (1 - y_i) \\cdot \\ln(1 - \\hat{y}_i + \\epsilon) \\right] $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10536051454671519\n"
     ]
    }
   ],
   "source": [
    "def binary_cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    バイナリ交差エントロピー損失関数\n",
    "\n",
    "    Args:\n",
    "        y_true (list): 正解値\n",
    "        y_pred (list): 予測値\n",
    "    \n",
    "    Returns:\n",
    "        float: バイナリ交差エントロピー損失\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: 入力リストの長さが異なる場合\n",
    "    \"\"\"\n",
    "    if len(y_true) != len(y_pred): raise ValueError(\"Input lists must have the same length.\")\n",
    "    epsilon = 1e-9  # 0で割るのを防ぐための小さな値\n",
    "    return -sum([t * ln(p + epsilon) + (1 - t) * ln(1 - p + epsilon) for t, p in zip(y_true, y_pred)]) / len(y_true)\n",
    "print(binary_cross_entropy_loss([0, 1], [0.1, 0.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### カテゴリカル交差エントロピー\n",
    "$$ L = -\\sum_{i=1}^{N} \\sum_{j=1}^{C} y_{ij} \\cdot \\ln(\\hat{y}_{ij} + \\epsilon) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.052680257273357595\n"
     ]
    }
   ],
   "source": [
    "def categorical_cross_entropy_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    カテゴリカル交差エントロピー損失関数\n",
    "\n",
    "    Args:\n",
    "        y_true (list): 正解値\n",
    "        y_pred (list): 予測値\n",
    "    \n",
    "    Returns:\n",
    "        float: カテゴリカル交差エントロピー損失\n",
    "    \n",
    "    Raises:\n",
    "        ValueError: 入力リストの長さが異なる場合\n",
    "    \"\"\"\n",
    "    if len(y_true) != len(y_pred): raise ValueError(\"Input lists must have the same length.\")\n",
    "    epsilon = 1e-9  # 0で割るのを防ぐための小さな値\n",
    "    return -sum([t * ln(p + epsilon) for t, p in zip(y_true, y_pred)]) / len(y_true)\n",
    "print(categorical_cross_entropy_loss([0, 1], [0.1, 0.9]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ニューラネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワークを初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[-0.6693492752147463, -0.14506312445752179], [0.8272344039416479, 0.07737687962166184], [-0.7106759682773515, -0.8118396127269876]], [[-0.3828822398075368, -0.27927042002645397, -1.020549424862728]]]\n"
     ]
    }
   ],
   "source": [
    "def initialize(layer_sizes):  # 重みとバイアスの初期化\n",
    "    \"\"\"\n",
    "    重みとバイアスを初期化する関数\n",
    "\n",
    "    Args:\n",
    "        layer_sizes (list): 各層のユニット数\n",
    "    \n",
    "    Returns:\n",
    "        tuple: 重みとバイアス\n",
    "    \"\"\"\n",
    "    weights, biases = [], []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        limit = sqrt(6 / (layer_sizes[i] + layer_sizes[i+1]))  # 重みの初期化に使う乱数の範囲\n",
    "        weights.append([[random.uniform(-limit, limit) for _ in range(layer_sizes[i])] for _ in range(layer_sizes[i+1])])  # 重みは -limit から limit の間の乱数で初期化\n",
    "        biases.append([0 for _ in range(layer_sizes[i+1])])  # バイアスは0で初期化\n",
    "    return weights, biases\n",
    "print(initialize([2, 3, 1])[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 順伝播処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(inputs, weights, biases, hidden_activation, output_activation):  # 順伝播処理\n",
    "    \"\"\"\n",
    "    順伝播処理を行う関数\n",
    "\n",
    "    Args:\n",
    "        inputs (list): 入力\n",
    "        weights (list): 重み\n",
    "        biases (list): バイアス\n",
    "    \n",
    "    Returns:\n",
    "        list: 出力\n",
    "    \"\"\"\n",
    "    activations = [inputs]\n",
    "    for W, b in zip(weights, biases):\n",
    "        z = [\n",
    "            sum([activations[-1][i] * W[j][i] for i in range(len(activations[-1]))]) + b[j]\n",
    "            for j in range(len(b))\n",
    "        ]\n",
    "        if W != weights[-1]:\n",
    "            activations.append([hidden_activation(z_i, derivative=False) for z_i in z])\n",
    "        else:\n",
    "            activations.append([output_activation(z_i, derivative=False) for z_i in z])\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逆伝播処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(activations, y_true, weights, biases, learning_rate, hidden_activation, output_activation):  # 逆伝播処理\n",
    "    \"\"\"\n",
    "    逆伝播処理を行う関数\n",
    "\n",
    "    Args:\n",
    "        activations (list): 出力\n",
    "        y_true (list): 正解ラベル\n",
    "        weights (list): 重み\n",
    "        biases (list): バイアス\n",
    "        learning_rate (float): 学習率\n",
    "    \n",
    "    Returns:\n",
    "        tuple: 重みとバイアス\n",
    "    \"\"\"\n",
    "    output_layer = activations[-1]\n",
    "    errors = [\n",
    "        (output_layer[i] - y_true[i]) * output_activation(output_layer[i], derivative=True)\n",
    "        for i in range(len(y_true))\n",
    "    ]\n",
    "    deltas = [errors]\n",
    "    # 隠れ層の誤差を計算\n",
    "    for l in range(len(weights)-1, 0, -1):\n",
    "        hidden_errors = [\n",
    "            sum([deltas[0][k] * weights[l][k][j] for k in range(len(deltas[0]))]) * hidden_activation(activations[l][j], derivative=True)\n",
    "            for j in range(len(activations[l]))\n",
    "        ]\n",
    "        deltas.insert(0, hidden_errors)\n",
    "    # 重みとバイアスを更新\n",
    "    for l in range(len(weights)):\n",
    "        for i in range(len(weights[l])):\n",
    "            for j in range(len(weights[l][i])):\n",
    "                weights[l][i][j] -= learning_rate * deltas[l][i] * activations[l][j]\n",
    "            biases[l][i] -= learning_rate * deltas[l][i]\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, layer_sizes, epochs, learning_rate, hidden_activation=relu, output_activation=sigmoid, loss=cross_entropy_loss):  # 学習\n",
    "    \"\"\"\n",
    "    ニューラルネットワークを学習する関数\n",
    "\n",
    "    Args:\n",
    "        X (list): 入力\n",
    "        y (list): 正解ラベル\n",
    "        layer_sizes (list): 各層のユニット数\n",
    "        epochs (int): エポック数\n",
    "        learning_rate (float): 学習率\n",
    "    \n",
    "    Returns:\n",
    "        tuple: 重みとバイアス\n",
    "    \"\"\"\n",
    "    weights, biases = initialize(layer_sizes)\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(len(X)):\n",
    "            activations = forward_propagation(X[i], weights, biases, hidden_activation, output_activation)\n",
    "            total_loss += loss(y[i], activations[-1])\n",
    "            weights, biases = backward_propagation(activations, y[i], weights, biases, learning_rate, hidden_activation, output_activation)\n",
    "        m = epoch // (epochs // 20) + 1\n",
    "        print(f\"\\rEpoch {epoch+1}/{epochs}, Loss: {total_loss/len(X):.10f}, [{'+'*m}{' '*(20-m)}]{' '*5}\",end=\"\")\n",
    "    print(f\"Training time: {time.time()-start:.2f} seconds\")\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, weights, biases, hidden_activation=relu, output_activation=sigmoid):  # 予測\n",
    "    \"\"\"\n",
    "    予測を行う関数\n",
    "\n",
    "    Args:\n",
    "        X (list): 入力\n",
    "        weights (list): 重み\n",
    "        biases (list): バイアス\n",
    "    \n",
    "    Returns:\n",
    "        list: 出力\n",
    "    \"\"\"\n",
    "    outputs = []\n",
    "    for i in range(len(X)):  # Prediction\n",
    "        outputs.append([0] if forward_propagation(X[i], weights, biases, hidden_activation, output_activation)[-1][0] < 0.5 else [1])\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 精度計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(X, y, predict, loss=cross_entropy_loss):  # 予測精度の計算\n",
    "    \"\"\"\n",
    "    予測精度を計算する関数\n",
    "\n",
    "    Args:\n",
    "        X (list): 入力\n",
    "        y (list): 正解ラベル\n",
    "        predict (list): 予測値\n",
    "    \n",
    "    Returns:\n",
    "        float: 予測精度\n",
    "    \"\"\"\n",
    "    total_loss = 0\n",
    "    for i in range(len(predict)):  # Prediction\n",
    "        print(f\"入力: {X[i]}, 正解: {y[i]}, 予測値: {predict[i]}\")\n",
    "        total_loss += loss(y[i], predict[i])\n",
    "    print(f\"Loss: {total_loss / len(predict):.10f}\")\n",
    "    return total_loss / len(predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データ処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 正規化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data, denomalize=False, min_val=None, max_val=None):\n",
    "    \"\"\"\n",
    "    データを正規化する関数\n",
    "\n",
    "    Args:\n",
    "        data (list): 入力データ\n",
    "        denomalize (bool): 逆正規化を行うかどうか\n",
    "        min_val (float): 最小値\n",
    "        max_val (float): 最大値\n",
    "    \n",
    "    Returns:\n",
    "        tuple: 正規化されたデータ、最小値、最大値もしくは逆正規化されたデータ\n",
    "    \"\"\"\n",
    "    if denomalize:\n",
    "        return [[x * (max_val - min_val) + min_val for x in sublist] for sublist in data]\n",
    "    min_val = min(min(sublist) for sublist in data)\n",
    "    max_val = max(max(sublist) for sublist in data)\n",
    "    nomalized_data = [[(x - min_val) / (max_val - min_val) for x in sublist] for sublist in data]\n",
    "    return nomalized_data, min_val, max_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 標準化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(data, unstandardize=False, mean=None, std_dev=None):\n",
    "    \"\"\"\n",
    "    データを標準化する関数\n",
    "\n",
    "    Args:\n",
    "        data (list): 入力データ\n",
    "        unstandardize (bool): 逆標準化を行うかどうか\n",
    "        mean (float): 平均\n",
    "        std_dev (float): 標準偏差\n",
    "    \n",
    "    Returns:\n",
    "        tuple: 標準化されたデータ、平均、標準偏差もしくは逆標準化されたデータ\n",
    "    \"\"\"\n",
    "    if unstandardize:\n",
    "        return [[x * std_dev + mean for x in sublist] for sublist in data]\n",
    "    mean = sum(sum(sublist) for sublist in data) / (len(data) * len(data[0]))\n",
    "    std_dev = (sum((x - mean) ** 2 for sublist in data for x in sublist) / (len(data) * len(data[0]))) ** 0.5\n",
    "    standardized_data = [[(x - mean) / std_dev for x in sublist] for sublist in data]\n",
    "    return standardized_data, mean, std_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ラベルエンコーディング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encoding(labels, decoding=False, label_to_index=None):\n",
    "    \"\"\"\n",
    "    ラベルエンコーディングを行う関数\n",
    "\n",
    "    Args:\n",
    "        labels (list): カテゴリカルデータのリスト(デコードを行う際はエンコードした元文字列(現数値)のみを指定可能)\n",
    "        decoding (bool): デコードを行うかどうか(デコードを行う際は label_to_index を指定)\n",
    "        label_to_index (dict): ラベルとインデックスのマッピング\n",
    "    \n",
    "    Returns:\n",
    "        tuple: エンコードされたラベル、ラベルとインデックスのマッピング\n",
    "    \"\"\"\n",
    "    if decoding:\n",
    "        return [[list(label_to_index.keys())[list(label_to_index.values()).index(label)] for label in sublist] for sublist in labels]\n",
    "    str_labels = [label for label in sum(labels, []) if type(label) == str]\n",
    "    label_to_index = {label: idx for idx, label in enumerate(sorted(set(str_labels)))}\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels[i])):\n",
    "            if labels[i][j] in label_to_index and type(labels[i][j]) == str:\n",
    "                labels[i][j] = label_to_index[labels[i][j]]\n",
    "    return labels, label_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ワンホットエンコーディング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encoding(labels, decoding=False, label_to_index=None):\n",
    "    \"\"\"\n",
    "    ワンホットエンコーディングを行う関数\n",
    "    \n",
    "    Args:\n",
    "        labels (list): カテゴリカルデータのリスト(デコードを行う際はエンコードした元文字列(現数値)のみを指定可能)\n",
    "        decoding (bool): デコードを行うかどうか(デコードを行う際は label_to_index を指定)\n",
    "        label_to_index (dict): ラベルとインデックスのマッピング\n",
    "    \n",
    "    Returns:\n",
    "        tuple: エンコードされたラベル、ラベルとインデックスのマッピング\n",
    "    \"\"\"\n",
    "    if decoding:\n",
    "        return[[k] for sublist in labels for i, x in enumerate(sublist) for k in label_to_index if x == 1 and i == label_to_index[k]]\n",
    "    str_labels = [label for label in sum(labels, []) if type(label) == str]\n",
    "    label_to_index = {label: idx for idx, label in enumerate(sorted(set(str_labels)))}\n",
    "    zeros = [0] * len(label_to_index)\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels[i])):\n",
    "            if labels[i][j] in label_to_index and type(labels[i][j]) == str:\n",
    "                labels[i][j] = zeros[:label_to_index[labels[i][j]]] + [1] + zeros[label_to_index[labels[i][j]]+1:]\n",
    "    return [[x for y in sublist for x in (y if isinstance(y, list) else [y])] for sublist in labels], label_to_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### データセット分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(X, y, train_size=0.8):  # データセットを学習用とテスト用に分割\n",
    "    \"\"\"\n",
    "    データセットを学習用とテスト用に分割する関数\n",
    "\n",
    "    Args:\n",
    "        X (list): 入力\n",
    "        y (list): 正解ラベル\n",
    "        train_size (float): 学習データの割合\n",
    "    \n",
    "    Returns:\n",
    "        tuple: 学習用データとテスト用データ\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    indices = list(range(n))\n",
    "    random.shuffle(indices)\n",
    "    X_train, y_train = [X[i] for i in indices[:int(n*train_size)]], [y[i] for i in indices[:int(n*train_size)]]\n",
    "    X_test, y_test = [X[i] for i in indices[int(n*train_size):]], [y[i] for i in indices[int(n*train_size):]]\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.0, 0, 22.0, 7.25], [1.0, 1, 38.0, 71.2833], [3.0, 1, 26.0, 7.925], [1.0, 1, 35.0, 53.1], [3.0, 0, 35.0, 8.05]]\n",
      "[[0], [1], [1], [1], [0]]\n"
     ]
    }
   ],
   "source": [
    "# データセットの読み込み\n",
    "with open('train.csv', newline='') as csvfile:\n",
    "    reader = csv.DictReader(csvfile)\n",
    "    titanic_data = [row for row in reader]\n",
    "\n",
    "# 学習に必要なカラムのみを抽出\n",
    "X = []\n",
    "y = []\n",
    "for row in titanic_data:\n",
    "    if row['Age'] and row['Fare']:  # 欠損値を含む行をスキップ\n",
    "        X.append([float(row['Pclass']), 0 if row['Sex'] == 'male' else 1, float(row['Age']), float(row['Fare'])])\n",
    "        y.append([int(row['Survived'])])\n",
    "\n",
    "print(X[:5])\n",
    "print(y[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メインコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 500/500, Loss: 0.2578774768, [++++++++++++++++++++]     Training time: 23.66 seconds\n",
      "入力: [2.0, 0, 27.0, 26.0], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 51.0, 7.0542], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 0, 42.0, 27.0], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 0, 38.0, 153.4625], 正解: [0], 予測値: [1]\n",
      "入力: [2.0, 1, 22.0, 41.5792], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 0, 29.0, 9.4833], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 0, 35.0, 10.5], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 36.0, 15.55], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 35.0, 8.05], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 1, 16.0, 7.7333], 正解: [1], 予測値: [0]\n",
      "入力: [3.0, 0, 25.0, 17.8], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 20.0, 15.7417], 正解: [1], 予測値: [0]\n",
      "入力: [3.0, 0, 21.0, 8.6625], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 35.0, 7.125], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 21.0, 7.925], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 0, 16.0, 26.0], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 36.0, 7.8958], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 21.0, 16.1], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 0, 25.0, 91.0792], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 1, 4.0, 16.7], 正解: [1], 予測値: [1]\n",
      "入力: [1.0, 1, 30.0, 106.425], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 0, 26.0, 8.05], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 19.0, 14.5], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 0, 50.0, 13.0], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 0, 47.0, 25.5875], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 1, 49.0, 76.7292], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 1, 27.0, 12.475], 正解: [1], 予測値: [0]\n",
      "入力: [3.0, 0, 27.0, 6.975], 正解: [1], 予測値: [0]\n",
      "入力: [2.0, 1, 57.0, 10.5], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 40.0, 7.225], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 21.0, 7.25], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 1, 8.0, 21.075], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 1, 30.0, 21.0], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 0, 6.0, 12.475], 正解: [1], 予測値: [0]\n",
      "入力: [1.0, 1, 50.0, 247.5208], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 0, 33.0, 7.8958], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 1, 36.0, 17.4], 正解: [1], 予測値: [0]\n",
      "入力: [3.0, 0, 20.0, 7.925], 正解: [1], 予測値: [0]\n",
      "入力: [2.0, 1, 35.0, 21.0], 正解: [1], 予測値: [0]\n",
      "入力: [1.0, 0, 71.0, 49.5042], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 0, 38.0, 0.0], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 0, 25.0, 55.4417], 正解: [1], 予測値: [0]\n",
      "入力: [3.0, 0, 40.0, 27.9], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 24.0, 16.1], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 1, 22.0, 7.775], 正解: [1], 予測値: [0]\n",
      "入力: [2.0, 0, 48.0, 13.0], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 1, 30.5, 7.75], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 1, 58.0, 26.55], 正解: [1], 予測値: [1]\n",
      "入力: [2.0, 1, 5.0, 27.75], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 0, 28.0, 7.8958], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 1, 19.0, 26.2833], 正解: [1], 予測値: [1]\n",
      "入力: [1.0, 1, 26.0, 78.85], 正解: [1], 予測値: [1]\n",
      "入力: [1.0, 1, 16.0, 57.9792], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 0, 32.0, 7.925], 正解: [1], 予測値: [0]\n",
      "入力: [2.0, 0, 2.0, 26.0], 正解: [1], 予測値: [0]\n",
      "入力: [1.0, 0, 35.0, 26.55], 正解: [1], 予測値: [0]\n",
      "入力: [1.0, 0, 46.0, 79.2], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 1, 24.0, 65.0], 正解: [1], 予測値: [0]\n",
      "入力: [3.0, 0, 30.5, 8.05], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 1, 24.0, 69.3], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 0, 2.0, 21.075], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 1, 42.0, 227.525], 正解: [1], 予測値: [1]\n",
      "入力: [2.0, 1, 41.0, 19.5], 正解: [1], 予測値: [0]\n",
      "入力: [2.0, 0, 39.0, 13.0], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 0, 23.0, 13.0], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 1, 26.0, 26.0], 正解: [0], 予測値: [1]\n",
      "入力: [3.0, 0, 26.0, 7.8542], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 0, 54.0, 14.0], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 1, 63.0, 9.5875], 正解: [1], 予測値: [0]\n",
      "入力: [1.0, 1, 15.0, 211.3375], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 0, 34.0, 8.05], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 1, 55.0, 16.0], 正解: [1], 予測値: [0]\n",
      "入力: [2.0, 0, 21.0, 73.5], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 0, 18.0, 11.5], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 30.0, 7.2292], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 39.0, 24.15], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 22.0, 7.25], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 39.0, 7.925], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 1, 26.0, 7.925], 正解: [1], 予測値: [0]\n",
      "入力: [2.0, 0, 34.0, 21.0], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 1, 14.0, 120.0], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 0, 4.0, 27.9], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 24.0, 8.05], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 1, 49.0, 25.9292], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 0, 26.0, 56.4958], 正解: [1], 予測値: [0]\n",
      "入力: [2.0, 1, 36.0, 13.0], 正解: [1], 予測値: [0]\n",
      "入力: [3.0, 1, 40.0, 9.475], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 1, 44.0, 27.7208], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 0, 9.0, 15.9], 正解: [1], 予測値: [0]\n",
      "入力: [2.0, 0, 59.0, 13.5], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 1, 29.0, 10.5], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 0, 25.0, 7.25], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 0, 4.0, 81.8583], 正解: [1], 予測値: [0]\n",
      "入力: [3.0, 1, 14.0, 11.2417], 正解: [1], 予測値: [0]\n",
      "入力: [2.0, 0, 42.0, 13.0], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 1, 18.0, 79.65], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 0, 55.5, 8.05], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 0, 38.0, 90.0], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 0, 16.0, 20.25], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 0, 23.0, 63.3583], 正解: [1], 予測値: [0]\n",
      "入力: [1.0, 0, 40.0, 31.0], 正解: [1], 予測値: [0]\n",
      "入力: [3.0, 0, 28.0, 7.8542], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 33.0, 20.525], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 22.0, 7.8958], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 1, 24.0, 27.0], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 0, 27.0, 8.6625], 正解: [1], 予測値: [0]\n",
      "入力: [3.0, 1, 19.0, 7.8792], 正解: [1], 予測値: [0]\n",
      "入力: [3.0, 0, 40.0, 7.8958], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 1, 5.0, 19.2583], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 0, 20.0, 8.05], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 32.0, 7.75], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 0, 29.0, 10.5], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 49.0, 0.0], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 1, 60.0, 75.25], 正解: [1], 予測値: [0]\n",
      "入力: [3.0, 1, 19.0, 7.8542], 正解: [1], 予測値: [0]\n",
      "入力: [2.0, 1, 18.0, 23.0], 正解: [1], 予測値: [1]\n",
      "入力: [1.0, 1, 56.0, 83.1583], 正解: [1], 予測値: [1]\n",
      "入力: [1.0, 0, 61.0, 33.5], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 1, 24.0, 18.75], 正解: [1], 予測値: [1]\n",
      "入力: [2.0, 0, 43.0, 26.25], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 16.0, 7.775], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 0, 64.0, 26.0], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 1, 2.0, 151.55], 正解: [0], 予測値: [1]\n",
      "入力: [3.0, 0, 19.0, 7.775], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 14.0, 46.9], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 0, 25.0, 13.0], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 0, 56.0, 30.6958], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 1, 24.0, 13.0], 正解: [0], 予測値: [1]\n",
      "入力: [3.0, 0, 32.0, 56.4958], 正解: [1], 予測値: [0]\n",
      "入力: [3.0, 0, 36.0, 0.0], 正解: [0], 予測値: [0]\n",
      "入力: [2.0, 0, 36.0, 10.5], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 0, 61.0, 32.3208], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 1, 22.0, 7.75], 正解: [1], 予測値: [0]\n",
      "入力: [1.0, 0, 48.0, 52.0], 正解: [1], 予測値: [0]\n",
      "入力: [3.0, 0, 38.0, 7.8958], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 0, 29.0, 30.0], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 0.42, 8.5167], 正解: [1], 予測値: [0]\n",
      "入力: [3.0, 0, 20.0, 9.8458], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 28.0, 7.8958], 正解: [0], 予測値: [0]\n",
      "入力: [1.0, 1, 30.0, 86.5], 正解: [1], 予測値: [1]\n",
      "入力: [3.0, 0, 15.0, 7.2292], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 0, 26.0, 20.575], 正解: [0], 予測値: [0]\n",
      "入力: [3.0, 1, 9.0, 15.2458], 正解: [0], 予測値: [1]\n",
      "Loss: 4.7822921160\n"
     ]
    }
   ],
   "source": [
    "# データセット\n",
    "# X = [[0, 0], [0, 1], [1, 0], [1, 1]]  # 入力\n",
    "# y = [[1], [0], [0], [1]]  # 出力\n",
    "\n",
    "# X = [[random.random(), random.random()] for _ in range(100)]\n",
    "# y = [[1] if x[0] + x[1] > 1 else [0] for x in X]\n",
    "\n",
    "# X = [[random.random(), random.random()] for _ in range(100)]\n",
    "# y = [[x[0] + x[1]] for x in X]\n",
    "\n",
    "# X = [[0, \"A\"], [1, \"A\"], [2, \"B\"], [3, \"A\"], [4, \"B\"], [5, \"A\"], [6, \"B\"], [7, \"B\"], [8, \"A\"], [9, \"B\"]]\n",
    "# y = [[1], [1], [0], [1], [0], [1], [0], [0], [1], [0]]\n",
    "\n",
    "# X = [[\"A\", \"A\"], [\"A\", \"A\"], [\"B\", \"B\"], [\"A\", \"A\"], [\"B\", \"B\"], [\"A\", \"A\"], [\"B\", \"B\"], [\"B\", \"B\"], [\"A\", \"A\"], [\"B\", \"B\"]]\n",
    "# y = [[1], [1], [0], [1], [0], [1], [0], [0], [1], [0]]\n",
    "\n",
    "# X, min_val, max_val = standardize(X)\n",
    "# X, label_to_index = label_encoding(X)\n",
    "\n",
    "X_train, y_train, X_test, y_test = split_dataset(X, y, train_size=0.8)\n",
    "\n",
    "epochs = 500  # エポック数\n",
    "learning_rate = 0.01  # 学習率\n",
    "layer_sizes = [len(X[0]), 8, 8, 8, len(y[0])]  # 各層のユニット数\n",
    "\n",
    "hidden_activation = relu\n",
    "output_activation = sigmoid\n",
    "loss = cross_entropy_loss\n",
    "\n",
    "# X_train: 入力, y_train: 出力, layer_sizes: 各層のユニット数, epochs: エポック数, learning_rate: 学習率, hidden_activation: 隠れ層の活性化関数, output_activation: 出力層の活性化関数, loss: 損失関数\n",
    "weights, biases = train(X_train, y_train, layer_sizes, epochs, learning_rate, hidden_activation=hidden_activation, output_activation=output_activation, loss=loss)\n",
    "# # X_test: テスト入力, weights: 重み, biases: バイアス, hidden_activation: 隠れ層の活性化関数, output_activation: 出力層の活性化関数\n",
    "predict_y = predict(X_test, weights, biases, hidden_activation=hidden_activation, output_activation=output_activation)\n",
    "\n",
    "# # X_test: テスト入力, y_test: テスト正解ラベル, predict_y: 予測値, loss: 損失関数\n",
    "accuracy_num = accuracy(X_test, y_test, predict_y, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 活性化関数\n",
    "\n",
    "### Sigmoid関数\n",
    "\n",
    "| **用途**                     | **隠れ層での使用**                                | **出力層での使用**                                       | **備考**                                                                 |\n",
    "|------------------------------|------------------------------------------------|-------------------------------------------------------|--------------------------------------------------------------------------|\n",
    "| **バイナリ分類**              | ×                                              | ◎ 確率を出力するために使用                              | 出力が0～1の範囲で、クラス1の確率として解釈可能。                          |\n",
    "| **多クラス分類（マルチラベル）** | △ 隠れ層で補助的に使用することも可能              | ◎ 各クラスの独立した確率出力に使用                      | 各クラスごとに個別の確率を算出する場合に適用（例: 複数ラベル分類）。         |\n",
    "| **多クラス分類（マルチクラス）** | ×                                              | × Softmax関数が主流                                    | 複数のクラスにまたがる分類タスクではSoftmaxが適している。                   |\n",
    "| **回帰問題**                  | △ 隠れ層で使用する場合もある                      | △ 出力を正規化する場合のみ使用可能                      | 出力範囲が0～1に制約される必要がある場合に限る。                           |\n",
    "| **時系列予測（ゲート構造）**   | ◎ RNNやLSTMのゲートで使用される                  | ×                                              | 入力ゲートや忘却ゲートなど、情報の取捨選択に有効。                         |\n",
    "| **確率的タスク**               | △ 特定の計算の一部として利用可能                  | ◎ 出力を確率として解釈する場合に使用                    | ベイズ的アプローチや確率的推論に適用可能。                                 |\n",
    "| **制約付き出力**               | △ 必要に応じて正規化に使用                        | ◎ 範囲を0～1に制限する場合に使用                        | 例: 需要予測、正規化された出力を要する経済学や物理学モデル。               |\n",
    "| **生物学的モデリング**         | ◎ ニューロン発火モデルなどで利用                  | △ 特定のタスクに限定                                   | 神経細胞の発火特性などS字型の挙動を模倣するタスクに有効。                   |\n",
    "\n",
    "### ReLU関数\n",
    "\n",
    "| **用途**                     | **隠れ層での使用**                                | **出力層での使用**                                       | **備考**                                                                 |\n",
    "|------------------------------|------------------------------------------------|-------------------------------------------------------|--------------------------------------------------------------------------|\n",
    "| **バイナリ分類**              | ◎                                              | ×                                                     | 出力層でのSigmoidの方が適しており、ReLUは主に隠れ層で使用される。           |\n",
    "| **多クラス分類（マルチラベル）** | ◎ 隠れ層での使用に適している                       | ×                                                     | 各クラスの確率出力にはSoftmaxやSigmoidが適しているため、ReLUは隠れ層で使用。 |\n",
    "| **多クラス分類（マルチクラス）** | ◎ 隠れ層での使用に最適                            | × Softmaxが主流                                        | 隠れ層での非線形性に優れたReLUが広く使われ、出力層にはSoftmaxを使用。         |\n",
    "| **回帰問題**                  | ◎ 隠れ層での使用に適している                       | △ 恒等関数（線形関数）が一般的                          | 出力層ではReLUを使うことは少ないが、正の値が重要な場合に使用されることがある。  |\n",
    "| **時系列予測（ゲート構造）**   | ◎ RNNやLSTMの隠れ層で広く使用                    | ×                                                     | 隠れ層での非線形性が強化され、勾配消失問題に対処可能。                     |\n",
    "| **確率的タスク**               | ◎ 隠れ層で使用されることが多い                      | ×                                                     | 出力層では確率を求めるためにSigmoidやSoftmaxが使われる。                   |\n",
    "| **制約付き出力**               | ◎ 出力が正の値のみ必要な場合に使用                  | × 出力範囲が制約される場合には他の関数を使用               | 正の値に限定される場合の隠れ層で非常に効果的。                             |\n",
    "| **生物学的モデリング**         | ◎ ニューロンの発火モデルで利用                      | ×                                                     | 活性化関数としては、生物学的ニューロンの特性に合う場合もあるが、一般にはReLUは隠れ層に使用。 |\n",
    "\n",
    "### Leaky ReLU関数\n",
    "\n",
    "| **用途**                     | **隠れ層での使用**                                | **出力層での使用**                                       | **備考**                                                                 |\n",
    "|------------------------------|------------------------------------------------|-------------------------------------------------------|--------------------------------------------------------------------------|\n",
    "| **バイナリ分類**              | ◎ 隠れ層で使用されることが多い                      | ×                                                     | 出力層でのSigmoidの方が適しており、Leaky ReLUは隠れ層で使用されることが一般的。 |\n",
    "| **多クラス分類（マルチラベル）** | ◎ 隠れ層で使用されることが多い                      | ×                                                     | SigmoidやSoftmaxが出力層で使用されるため、Leaky ReLUは主に隠れ層で利用される。 |\n",
    "| **多クラス分類（マルチクラス）** | ◎ 隠れ層での使用に最適                            | × Softmaxが主流                                        | 出力層ではSoftmax、隠れ層ではLeaky ReLUが広く使われる。                   |\n",
    "| **回帰問題**                  | ◎ 隠れ層で使用されることが多い                      | △ 恒等関数（線形関数）が一般的                          | 出力層ではReLUや恒等関数を使うことが一般的だが、Leaky ReLUは隠れ層で使われる。  |\n",
    "| **時系列予測（ゲート構造）**   | ◎ RNNやLSTMの隠れ層で使用されることがある           | ×                                                     | 勾配消失問題に強いため、LSTMやGRUのようなRNNアーキテクチャで有効。        |\n",
    "| **確率的タスク**               | ◎ 隠れ層で使用されることが多い                      | ×                                                     | 出力層で確率を求めるためにはSigmoidやSoftmaxが使用される。                   |\n",
    "| **制約付き出力**               | ◎ 出力が正の値のみ必要な場合に使用                  | × 出力範囲が制約される場合には他の関数を使用               | 正の値が重要な場合に、隠れ層でReLUの代わりに使用されることが多い。            |\n",
    "| **生物学的モデリング**         | ◎ ニューロンの発火モデルで利用                      | ×                                                     | ReLUよりも小さな負の出力を許容するため、発火モデルにおいても有効。            |\n",
    "\n",
    "### Identity関数（恒等関数）\n",
    "\n",
    "| **用途**                     | **隠れ層での使用**                                | **出力層での使用**                                       | **備考**                                                                 |\n",
    "|------------------------------|------------------------------------------------|-------------------------------------------------------|--------------------------------------------------------------------------|\n",
    "| **バイナリ分類**              | ×                                              | ×                                                     | 恒等関数はバイナリ分類では通常使用されません。出力層にはSigmoidが適している。 |\n",
    "| **多クラス分類（マルチラベル）** | ×                                              | ×                                                     | 出力層にはSoftmaxやSigmoidが使用されるため、恒等関数は使用されません。         |\n",
    "| **多クラス分類（マルチクラス）** | ×                                              | ×                                                     | 出力層にはSoftmaxが主流であり、恒等関数は使用されません。                     |\n",
    "| **回帰問題**                  | ◎ 隠れ層での使用が稀にあるが、通常は他の関数（ReLUなど）を使用 | ◎ 回帰問題では出力層で最も一般的に使用される               | 恒等関数は回帰問題の出力層で広く使用される。値の範囲に制約がないため、連続的な数値予測に最適。 |\n",
    "| **時系列予測（ゲート構造）**   | ×                                              | ×                                                     | 時系列予測においては、ReLUやLSTMなど他の関数が使用される。                     |\n",
    "| **確率的タスク**               | ×                                              | ×                                                     | 確率出力が求められるタスクにはSigmoidやSoftmaxが使用されるため、恒等関数は使用されません。 |\n",
    "| **制約付き出力**               | ×                                              | △ 出力が特定の範囲に収まる必要がある場合に使用されることがある  | 恒等関数は制約なしに連続的な出力を生成するため、特定の範囲制約がない場合に使用されます。 |\n",
    "| **生物学的モデリング**         | ×                                              | ×                                                     | 生物学的なモデルや発火モデルなどでは使用されません。                          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 損失関数の説明\n",
    "\n",
    "### Cross Entropy Loss（クロスエントロピー誤差）\n",
    "\n",
    "| **用途**                     | **定義**                                           | **適用される問題**                                   | **活性化関数**                           | **備考**                                                                 |\n",
    "|------------------------------|--------------------------------------------------|----------------------------------------------------|-----------------------------------------|--------------------------------------------------------------------------|\n",
    "| **バイナリ分類**              | 2クラス（0または1）に対する予測と実際のラベルの間の誤差 | バイナリ分類問題                                    | 出力層: Sigmoid                        | 出力が0～1の確率で、クラス1の確率を予測する。                             |\n",
    "| **多クラス分類（マルチクラス）** | 複数クラスの中から正解クラスを予測する誤差             | マルチクラス分類問題                                | 出力層: Softmax                        | ソフトマックス関数を用いて、各クラスに対する確率を出力し、最大の確率を持つクラスが選ばれる。 |\n",
    "| **多クラス分類（マルチラベル）** | 複数のラベルを同時に予測する誤差                     | 各ラベルが独立した多クラス分類問題（複数ラベル）    | 出力層: Sigmoid                        | 各クラスごとに独立した確率が出力され、複数のクラスを同時に予測する場合に使用される。 |\n",
    "| **回帰問題**                  | 使用されない                                      | 回帰問題                                            | 恒等関数（Identity）または他の回帰関数 | 回帰問題ではCross Entropy Lossは一般的に使用されません。                |\n",
    "| **確率的タスク**               | 出力確率を求めるタスクで、予測と実際の確率の誤差を計算 | 確率的な予測が必要なタスク（例: パラメータ推定）    | 出力層: SigmoidまたはSoftmax           | 確率的なタスクや予測問題で誤差を最小化するために使用される。               |\n",
    "| **ニューラルネットワークのトレーニング** | モデルが予測する確率と実際のラベルとの誤差を計算     | ネットワークの学習中の誤差計算                      | 出力層: SigmoidまたはSoftmax           | クロスエントロピー誤差は多くの分類問題の損失関数として標準的に使用される。 |\n",
    "\n",
    "### Mean Squared Error (MSE)\n",
    "| **用途**               | **定義**                                          | **適用される問題**                          | **活性化関数**            | **備考**                                                                 |\n",
    "|------------------------|-------------------------------------------------|-------------------------------------------|--------------------------|--------------------------------------------------------------------------|\n",
    "| **回帰問題**            | 実際の値と予測値の二乗誤差の平均                    | 回帰問題（連続的な数値予測）                | 恒等関数（Identity）       | MSEは回帰問題でよく使用され、誤差が大きい予測に対してペナルティを強く与えます。   |\n",
    "| **ノイズ除去**          | ノイズ除去タスクで、実際の出力と予測出力との差の二乗平均  | 信号処理や画像のノイズ除去など               | 恒等関数（Identity）       | 特にノイズが含まれるデータの学習において、予測誤差を最小化するために使用されます。 |\n",
    "\n",
    "### Mean Absolute Error (MAE)\n",
    "| **用途**               | **定義**                                          | **適用される問題**                          | **活性化関数**            | **備考**                                                                 |\n",
    "|------------------------|-------------------------------------------------|-------------------------------------------|--------------------------|--------------------------------------------------------------------------|\n",
    "| **回帰問題**            | 実際の値と予測値の絶対誤差の平均                    | 回帰問題（連続的な数値予測）                | 恒等関数（Identity）       | MAEは外れ値に敏感でなく、誤差が小さい場合に使いやすい損失関数です。        |\n",
    "| **ロバスト回帰**        | 外れ値に対して頑健な回帰問題                     | 外れ値が含まれる回帰問題                    | 恒等関数（Identity）       | MSEが外れ値に強く反応するのに対し、MAEは外れ値に対してロバストです。      |\n",
    "\n",
    "### Binary Cross-Entropy Loss\n",
    "| **用途**               | **定義**                                          | **適用される問題**                          | **活性化関数**            | **備考**                                                                 |\n",
    "|------------------------|-------------------------------------------------|-------------------------------------------|--------------------------|--------------------------------------------------------------------------|\n",
    "| **バイナリ分類**        | 2クラス（0または1）の予測と実際のラベルの間の誤差 | バイナリ分類問題（例: スパム分類）            | 出力層: Sigmoid           | 出力が0〜1の確率として解釈され、最も確信度の高いクラスを選択する。         |\n",
    "| **確率的推定**          | 予測確率と実際のラベルの間の誤差を計算           | 確率的な分類タスク（例: 二項分布のパラメータ推定）| 出力層: Sigmoid           | 確率的な推定やパラメータ推定など、確率を扱うタスクに使用されます。          |\n",
    "\n",
    "### Categorical Cross-Entropy Loss\n",
    "| **用途**               | **定義**                                          | **適用される問題**                          | **活性化関数**            | **備考**                                                                 |\n",
    "|------------------------|-------------------------------------------------|-------------------------------------------|--------------------------|--------------------------------------------------------------------------|\n",
    "| **多クラス分類**        | 複数クラスの中から正解クラスを予測する誤差        | 多クラス分類問題（例: 手書き数字認識）        | 出力層: Softmax           | ソフトマックス関数を使用し、各クラスの確率を出力。最大確率を持つクラスが選ばれる。 |\n",
    "| **確率的推定**          | 各クラスの確率と実際のラベルとの誤差を計算       | 確率的な分類タスク（例: 多項分布のパラメータ推定）| 出力層: Softmax           | 確率的な分類タスクやパラメータ推定で使用され、出力確率を最大化します。         |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
