{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.util.stream.IntStream;\n",
    "import java.util.ArrayList;\n",
    "import java.util.List;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "/**\n",
    " * ネイピア数を求める関数\n",
    " * \n",
    " * @param x ネイピア数の底\n",
    " * @return ネイピア数\n",
    " */\n",
    "double napiersLogarithm(double x) {\n",
    "    return Math.pow((1 + 1 / x), x);\n",
    "}\n",
    "double napierNumber = napiersLogarithm(100000000.0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "public class ActivationFunction {\n",
    "    /**\n",
    "     * Sigmoid 関数およびその微分\n",
    "     * \n",
    "     * @param x 入力\n",
    "     * @param derivative true の場合は Sigmoid 関数の微分を返す\n",
    "     * @return Sigmoid 関数の値またはその微分\n",
    "     */\n",
    "    public double sigmoid(double x, boolean derivative) {\n",
    "        if (derivative) {\n",
    "            double sigmoidValue = 1 / (1 + Math.exp(-x));\n",
    "            return sigmoidValue * (1 - sigmoidValue);\n",
    "        }\n",
    "        return 1 / (1 + Math.exp(-x));\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * ReLU 関数およびその微分\n",
    "     * \n",
    "     * @param x 入力\n",
    "     * @param derivative true の場合は ReLU 関数の微分を返す\n",
    "     * @return ReLU 関数の値またはその微分\n",
    "     */\n",
    "    public double relu(double x, boolean derivative) {\n",
    "        if (derivative) {\n",
    "            return x > 0 ? 1 : 0;\n",
    "        }\n",
    "        return Math.max(0, x);\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * Leaky ReLU 関数およびその微分\n",
    "     * \n",
    "     * @param x 入力\n",
    "     * @param derivative true の場合は Leaky ReLU 関数の微分を返す\n",
    "     * @return Leaky ReLU 関数の値またはその微分\n",
    "     */\n",
    "    public double leakyRelu(double x, boolean derivative) {\n",
    "        double alpha = 0.01;\n",
    "        if (derivative) {\n",
    "            return x > 0 ? 1 : alpha;\n",
    "        }\n",
    "        return x > 0 ? x : alpha * x;\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * 恒等関数およびその微分\n",
    "     * \n",
    "     * @param x 入力\n",
    "     * @param derivative true の場合は恒等関数の微分を返す\n",
    "     * @return 恒等関数の値またはその微分\n",
    "     */\n",
    "    public double identity(double x, boolean derivative) {\n",
    "        if (derivative) {\n",
    "            return 1;\n",
    "        }\n",
    "        return x;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "public class LossFunction{\n",
    "    /**\n",
    "     * 交差エントロピー損失関数\n",
    "     * \n",
    "     * @param yTrue 正解値\n",
    "     * @param yPred 予測値\n",
    "     * @return 交差エントロピー損失\n",
    "     * @throws IllegalArgumentException 入力リストの長さが異なる場合\n",
    "     */\n",
    "    double crossEntropyError(double[] yTrue, double[] yPred) {\n",
    "        if (yTrue.length != yPred.length) {\n",
    "            throw new IllegalArgumentException(\"Input lists must have the same length.\");\n",
    "        }\n",
    "        return -IntStream.range(0, yTrue.length)\n",
    "                            .mapToDouble(i -> yTrue[i] * Math.log(yPred[i] + 1e-9))\n",
    "                            .sum();\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * 平均二乗誤差関数\n",
    "     * \n",
    "     * @param yTrue 正解値\n",
    "     * @param yPred 予測値\n",
    "     * @return 平均二乗誤差\n",
    "     * @throws IllegalArgumentException 入力リストの長さが異なる場合\n",
    "     */\n",
    "    double meanSquaredError(double[] yTrue, double[] yPred) {\n",
    "        if (yTrue.length != yPred.length) {\n",
    "            throw new IllegalArgumentException(\"Input lists must have the same length.\");\n",
    "        }\n",
    "        return IntStream.range(0, yTrue.length)\n",
    "                        .mapToDouble(i -> Math.pow(yTrue[i] - yPred[i], 2))\n",
    "                        .average()\n",
    "                        .orElse(Double.NaN);\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * 平均絶対誤差関数\n",
    "     * \n",
    "     * @param yTrue 正解値\n",
    "     * @param yPred 予測値\n",
    "     * @return 平均絶対誤差\n",
    "     * @throws IllegalArgumentException 入力リストの長さが異なる場合\n",
    "     */\n",
    "    double meanAbsoluteError(double[] yTrue, double[] yPred) {\n",
    "        if (yTrue.length != yPred.length) {\n",
    "            throw new IllegalArgumentException(\"Input lists must have the same length.\");\n",
    "        }\n",
    "        return IntStream.range(0, yTrue.length)\n",
    "                        .mapToDouble(i -> Math.abs(yTrue[i] - yPred[i]))\n",
    "                        .average()\n",
    "                        .orElse(Double.NaN);\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * バイナリ交差エントロピー損失関数\n",
    "     * \n",
    "     * @param yTrue 正解値\n",
    "     * @param yPred 予測値\n",
    "     * @return バイナリ交差エントロピー損失\n",
    "     * @throws IllegalArgumentException 入力リストの長さが異なる場合\n",
    "     */\n",
    "    double binaryCrossEntropy(double[] yTrue, double[] yPred) {\n",
    "        if (yTrue.length != yPred.length) {\n",
    "            throw new IllegalArgumentException(\"Input lists must have the same length.\");\n",
    "        }\n",
    "        double epsilon = 1e-9;\n",
    "        return -IntStream.range(0, yTrue.length)\n",
    "                            .mapToDouble(i -> yTrue[i] * Math.log(yPred[i] + epsilon) + (1 - yTrue[i]) * Math.log(1 - yPred[i] + epsilon))\n",
    "                            .sum() / yTrue.length;\n",
    "    }\n",
    "\n",
    "    /**\n",
    "     * カテゴリカル交差エントロピー損失関数\n",
    "     * \n",
    "     * @param yTrue 正解値\n",
    "     * @param yPred 予測値\n",
    "     * @return カテゴリカル交差エントロピー損失\n",
    "     * @throws IllegalArgumentException 入力リストの長さが異なる場合\n",
    "     */\n",
    "    double categoricalCrossEntropy(double[] yTrue, double[] yPred) {\n",
    "        if (yTrue.length != yPred.length) {\n",
    "            throw new IllegalArgumentException(\"Input lists must have the same length.\");\n",
    "        }\n",
    "        double epsilon = 1e-9;\n",
    "        return -IntStream.range(0, yTrue.length)\n",
    "                            .mapToDouble(i -> yTrue[i] * Math.log(yPred[i] + epsilon))\n",
    "                            .sum() / yTrue.length;\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "public class NeuralNetwork {\n",
    "    // 学習メソッド\n",
    "    public static void train(List<List<Double>> X, List<List<Double>> y, int[] layerSizes, int epochs, double learningRate, ActivationFunction hiddenActivation, ActivationFunction outputActivation, LossFunction lossFunction) {\n",
    "        // 重みとバイアスの初期化\n",
    "        double[][][] weights = initializeWeights(layerSizes);\n",
    "        double[][] biases = initializeBiases(layerSizes);\n",
    "\n",
    "        for (int epoch = 0; epoch < epochs; epoch++) {\n",
    "            double totalLoss = 0;\n",
    "\n",
    "            // 各データに対する学習\n",
    "            for (int i = 0; i < X.size(); i++) {\n",
    "                // 順伝播\n",
    "                List<List<Double>> activations = forwardPropagation(X.get(i), weights, biases, hiddenActivation, outputActivation);\n",
    "\n",
    "                // 損失計算\n",
    "                double[] yTrue = y.get(i).stream().mapToDouble(Double::doubleValue).toArray();\n",
    "                double[] yPred = activations.get(activations.size() - 1).stream().mapToDouble(Double::doubleValue).toArray();\n",
    "                totalLoss += lossFunction.crossEntropyError(yTrue, yPred);\n",
    "\n",
    "                // 逆伝播\n",
    "                backwardPropagation(activations, y.get(i), weights, biases, learningRate, hiddenActivation, outputActivation);\n",
    "            }\n",
    "\n",
    "            // エポックごとの損失表示\n",
    "            if (epoch % 10 == 0 || epoch == epochs - 1) {\n",
    "                int m = epoch / (epochs / 20) + 1;\n",
    "                String memory = \"\";\n",
    "                for (int j = 0; j < m; j++) {\n",
    "                    memory += \"+\";\n",
    "                }\n",
    "                String space = \"\";\n",
    "                for (int j = 0; j < 20 - m; j++) {\n",
    "                    space += \" \";\n",
    "                }\n",
    "                System.out.print(\"\\rEpoch \" + (epoch + 1) + \"/\" + epochs + \", Loss: \" + totalLoss / X.size() + \", [\" + memory + space + \"]             \");\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    // 重みの初期化\n",
    "    private static double[][][] initializeWeights(int[] layerSizes) {\n",
    "        Random random = new Random();\n",
    "        double[][][] weights = new double[layerSizes.length - 1][][];\n",
    "\n",
    "        for (int layer = 0; layer < layerSizes.length - 1; layer++) {\n",
    "            weights[layer] = new double[layerSizes[layer]][layerSizes[layer + 1]];\n",
    "            double variance = 2.0 / (layerSizes[layer] + layerSizes[layer + 1]);\n",
    "            for (int i = 0; i < layerSizes[layer]; i++) {\n",
    "                for (int j = 0; j < layerSizes[layer + 1]; j++) {\n",
    "                    weights[layer][i][j] = random.nextGaussian() * Math.sqrt(variance);\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        return weights;\n",
    "    }\n",
    "\n",
    "    // バイアスの初期化\n",
    "    private static double[][] initializeBiases(int[] layerSizes) {\n",
    "        double[][] biases = new double[layerSizes.length - 1][];\n",
    "\n",
    "        for (int layer = 0; layer < layerSizes.length - 1; layer++) {\n",
    "            biases[layer] = new double[layerSizes[layer + 1]];\n",
    "            for (int i = 0; i < layerSizes[layer + 1]; i++) {\n",
    "                biases[layer][i] = 0; // Xavier初期化ではバイアスは0に初期化\n",
    "            }\n",
    "        }\n",
    "        return biases;\n",
    "    }\n",
    "\n",
    "    // 順伝播\n",
    "    public static List<List<Double>> forwardPropagation(List<Double> input, double[][][] weights, double[][] biases, ActivationFunction hiddenActivation, ActivationFunction outputActivation) {\n",
    "        List<List<Double>> activations = new ArrayList<>();\n",
    "        List<Double> currentLayer = new ArrayList<>(input);\n",
    "        activations.add(currentLayer);\n",
    "\n",
    "        for (int layer = 0; layer < weights.length; layer++) {\n",
    "            List<Double> nextLayer = new ArrayList<>();\n",
    "            for (int neuron = 0; neuron < weights[layer][0].length; neuron++) {\n",
    "                double z = 0;\n",
    "                for (int prevNeuron = 0; prevNeuron < currentLayer.size(); prevNeuron++) {\n",
    "                    z += currentLayer.get(prevNeuron) * weights[layer][prevNeuron][neuron];\n",
    "                }\n",
    "                z += biases[layer][neuron];\n",
    "\n",
    "                double activation = (layer == weights.length - 1)\n",
    "                        ? outputActivation.sigmoid(z, false)\n",
    "                        : hiddenActivation.relu(z, false);\n",
    "                nextLayer.add(activation);\n",
    "            }\n",
    "            currentLayer = nextLayer;\n",
    "            activations.add(currentLayer);\n",
    "        }\n",
    "\n",
    "        return activations;\n",
    "    }\n",
    "\n",
    "    // 逆伝播\n",
    "    public static void backwardPropagation(List<List<Double>> activations, List<Double> yTrue, double[][][] weights, double[][] biases, double learningRate, ActivationFunction hiddenActivation, ActivationFunction outputActivation) {\n",
    "        int numLayers = weights.length;\n",
    "        double[][] delta = new double[numLayers][];\n",
    "\n",
    "        // 出力層のデルタ計算\n",
    "        List<Double> output = activations.get(numLayers);\n",
    "        delta[numLayers - 1] = new double[output.size()];\n",
    "        for (int i = 0; i < output.size(); i++) {\n",
    "            double error = output.get(i) - yTrue.get(i);\n",
    "            delta[numLayers - 1][i] = error * outputActivation.sigmoid(output.get(i), true);\n",
    "        }\n",
    "\n",
    "        // 隠れ層のデルタ計算\n",
    "        for (int layer = numLayers - 2; layer >= 0; layer--) {\n",
    "            delta[layer] = new double[weights[layer][0].length];\n",
    "            for (int i = 0; i < weights[layer][0].length; i++) {\n",
    "                double error = 0;\n",
    "                for (int j = 0; j < weights[layer + 1][i].length; j++) {\n",
    "                    error += delta[layer + 1][j] * weights[layer + 1][i][j];\n",
    "                }\n",
    "                delta[layer][i] = error * hiddenActivation.relu(activations.get(layer + 1).get(i), true);\n",
    "            }\n",
    "        }\n",
    "\n",
    "        // 重みとバイアスの更新\n",
    "        for (int layer = 0; layer < numLayers; layer++) {\n",
    "            for (int i = 0; i < weights[layer].length; i++) {\n",
    "                for (int j = 0; j < weights[layer][i].length; j++) {\n",
    "                    weights[layer][i][j] -= learningRate * activations.get(layer).get(i) * delta[layer][j];\n",
    "                }\n",
    "            }\n",
    "            for (int i = 0; i < biases[layer].length; i++) {\n",
    "                biases[layer][i] -= learningRate * delta[layer][i];\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "List<List<Double>> X = Arrays.asList(\n",
    "        Arrays.asList(0.0, 0.0),\n",
    "        Arrays.asList(0.0, 1.0),\n",
    "        Arrays.asList(1.0, 0.0),\n",
    "        Arrays.asList(1.0, 1.0)\n",
    ");\n",
    "\n",
    "List<List<Double>> y = Arrays.asList(\n",
    "        Arrays.asList(1.0),\n",
    "        Arrays.asList(0.0),\n",
    "        Arrays.asList(0.0),\n",
    "        Arrays.asList(1.0)\n",
    ");\n",
    "\n",
    "// パラメータ設定\n",
    "int[] layerSizes = {2, 8, 8, 8, 1};\n",
    "int epochs = 100000;\n",
    "double learningRate = 0.01;\n",
    "\n",
    "// 活性化関数と損失関数のインスタンス化\n",
    "ActivationFunction hiddenActivation = new ActivationFunction();\n",
    "ActivationFunction outputActivation = new ActivationFunction();\n",
    "LossFunction lossFunction = new LossFunction();\n",
    "\n",
    "NeuralNetwork.train(X, y, layerSizes, epochs, learningRate, hiddenActivation, outputActivation, lossFunction);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "java",
   "pygments_lexer": "java",
   "version": "17.0.13+11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
