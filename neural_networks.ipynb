{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モジュールをインポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 707,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ネイピア数\n",
    "$$e = \\lim_{{x \\to \\infty}} \\left(1 + \\frac{1}{x}\\right)^x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 708,
   "metadata": {},
   "outputs": [],
   "source": [
    "def napiers_logarithm(x):\n",
    "    return (1 + 1 / x) ** x\n",
    "napier_number = napiers_logarithm(100000000)  # e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### シグモイド関数\n",
    "$$Sigmoid(x) = \\frac{1}{1 + e^{-x}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 709,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + napier_number ** -x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### シグモイド関数の微分\n",
    "$$Sigmoid'(x) = Sigmoid(x) \\cdot (1 - Sigmoid(x))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 710,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU関数\n",
    "$$ReLU(x) = \\max(0, x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 711,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return max(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU関数の微分\n",
    "$$ReLU'(x) = \\begin{cases} \n",
    "1 & (x > 0) \\\\\n",
    "0 & (x ≤ 0)\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 712,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_derivative(x):\n",
    "    return 1 if x > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自然対数\n",
    "\n",
    "1. **スケーリング**  \n",
    "   自然対数の性質 $\\ln(a \\cdot b) = \\ln(a) + \\ln(b)$ を利用して、$x$ を 1 に近い値に変換\n",
    "\n",
    "   $$\n",
    "   k = 0, \\quad x > 2 \\text{ の間 } x = \\frac{x}{2}, \\, k += 1\n",
    "   $$\n",
    "\n",
    "   $$\n",
    "   x < 0.5 \\text{ の間 } x = 2 \\cdot x, \\, k -= 1\n",
    "   $$\n",
    "\n",
    "   変換後、$x \\in [0.5, 2]$ に収まる\n",
    "\n",
    "2. **ニュートン法**  \n",
    "   方程式 $f(y) = e^y - x = 0$ を解くため、ニュートン法を適用\n",
    "\n",
    "   $$\n",
    "   y_{n+1} = y_n - \\frac{e^{y_n} - x}{e^{y_n}}\n",
    "   $$\n",
    "\n",
    "   初期値として $y_0 = x - 1$ を用いる。\n",
    "\n",
    "3. **結果の計算**  \n",
    "   最終的に、結果は次の式で求められます：\n",
    "\n",
    "   $$\n",
    "   \\ln(x) = y + k \\cdot \\ln(2)\n",
    "   $$\n",
    "\n",
    "   ここで、$\\ln(2) \\approx 0.6931471805599453$ を用います。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 713,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ln(x, max_iter=20, tol=1e-12):\n",
    "    if x <= 0: raise ValueError(\"x must be positive\")\n",
    "\n",
    "    # 初期化: x を 1 に近づけるためのスケーリング\n",
    "    k = 0\n",
    "    while x > 2:\n",
    "        x /= 2\n",
    "        k += 1\n",
    "    while x < 0.5:\n",
    "        x *= 2\n",
    "        k -= 1\n",
    "    # 初期値（ニュートン法用）\n",
    "    y = x - 1  # ln(1) = 0 付近の値から開始\n",
    "\n",
    "    # ニュートン法を適用\n",
    "    for _ in range(max_iter):\n",
    "        prev_y = y\n",
    "        y -= (2.718281828459045**y - x) / (2.718281828459045**y)  # f(y) / f'(y)\n",
    "        if abs(y - prev_y) < tol:\n",
    "            break\n",
    "\n",
    "    return y + k * 0.6931471805599453  # ln(2) ≈ 0.693147\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### クロスエントロピー損失\n",
    "$$ L = -\\sum_{i=1}^{N} y_i \\cdot \\ln(\\hat{y}_i + \\epsilon)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss(y_true, y_pred):\n",
    "    if len(y_true) != len(y_pred): raise ValueError(\"Input lists must have the same length.\")\n",
    "    return -sum([t * ln(p + 1e-9) for t, p in zip(y_true, y_pred)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ニューラルネットワークを初期化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_weights(layer_sizes):  # 重みとバイアスの初期化\n",
    "    weights, biases = [], []\n",
    "    for i in range(len(layer_sizes) - 1):\n",
    "        weights.append([[random.uniform(-1, 1) for _ in range(layer_sizes[i])] for _ in range(layer_sizes[i+1])])\n",
    "        biases.append([random.uniform(-1, 1) for _ in range(layer_sizes[i+1])])\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 順伝播処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 716,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_propagation(inputs, weights, biases):  # 順伝播処理\n",
    "    activations = [inputs]\n",
    "    for W, b in zip(weights, biases):\n",
    "        z = [\n",
    "            sum([activations[-1][i] * W[j][i] for i in range(len(activations[-1]))]) + b[j]\n",
    "            for j in range(len(b))\n",
    "        ]\n",
    "        if W != weights[-1]:\n",
    "            activations.append([relu(z_i) for z_i in z])\n",
    "        else:\n",
    "            activations.append([sigmoid(z_i) for z_i in z])\n",
    "    return activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 逆伝播処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 717,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_propagation(activations, y_true, weights, biases, learning_rate):  # 逆伝播処理\n",
    "    output_layer = activations[-1]\n",
    "    errors = [\n",
    "        (output_layer[i] - y_true[i]) * sigmoid_derivative(output_layer[i])\n",
    "        for i in range(len(y_true))\n",
    "    ]\n",
    "    deltas = [errors]\n",
    "    # Backpropagating the hidden layer error\n",
    "    for l in range(len(weights)-1, 0, -1):\n",
    "        hidden_errors = [\n",
    "            sum([deltas[0][k] * weights[l][k][j] for k in range(len(deltas[0]))]) * relu_derivative(activations[l][j])\n",
    "            for j in range(len(activations[l]))\n",
    "        ]\n",
    "        deltas.insert(0, hidden_errors)\n",
    "    # Update the weights and biases\n",
    "    for l in range(len(weights)):\n",
    "        for i in range(len(weights[l])):\n",
    "            for j in range(len(weights[l][i])):\n",
    "                weights[l][i][j] -= learning_rate * deltas[l][i] * activations[l][j]\n",
    "            biases[l][i] -= learning_rate * deltas[l][i]\n",
    "\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X, y, layer_sizes, epochs, learning_rate):  # 学習\n",
    "    weights, biases = initialize_weights(layer_sizes)\n",
    "    start = time.time()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for i in range(len(X)):\n",
    "            activations = forward_propagation(X[i], weights, biases)\n",
    "            total_loss += cross_entropy_loss(y[i], activations[-1])\n",
    "            weights, biases = backward_propagation(activations, y[i], weights, biases, learning_rate)\n",
    "        m = epoch // (epochs // 20) + 1\n",
    "        print(f\"\\rEpoch {epoch+1}/{epochs}, Loss: {total_loss/len(X):.10f}, [{'+'*m}{' '*(20-m)}]{' '*5}\",end=\"\")\n",
    "    print(f\"Training time: {time.time()-start:.2f} seconds\")\n",
    "    return weights, biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### メインコード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000, Loss: 0.0195980323, [++++++++++++++++++++]     Training time: 0.58 seconds\n",
      "Inputs: [0, 0], Output: [0] Predict: [0], probability: 0.02\n",
      "Inputs: [0, 1], Output: [1] Predict: [1], probability: 0.958\n",
      "Inputs: [1, 0], Output: [1] Predict: [1], probability: 0.966\n",
      "Inputs: [1, 1], Output: [0] Predict: [0], probability: 0.034\n"
     ]
    }
   ],
   "source": [
    "# DataSet for XOR\n",
    "X = [[0, 0], [0, 1], [1, 0], [1, 1]]  # Input\n",
    "y = [[0], [1], [1], [0]]  # Output\n",
    "\n",
    "epochs = 1000  # Number of epochs\n",
    "learning_rate = 0.01  # Learning rate\n",
    "layer_sizes = [2, 8, 16, 8, 1]  # 2 input -> 8 hidden -> 16 hidden -> 8 hidden -> 1 output\n",
    "\n",
    "weights, biases = train(X, y, layer_sizes, epochs, learning_rate)\n",
    "\n",
    "for i in range(len(X)):  # Prediction\n",
    "    activations = forward_propagation(X[i], weights, biases)\n",
    "    output = activations[-1]\n",
    "    binary_output = [1 if o >= 0.5 else 0 for o in output]\n",
    "    print(f\"Inputs: {X[i]}, Output: {y[i]} Predict: {binary_output}, probability: {round(output[0], 3)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
